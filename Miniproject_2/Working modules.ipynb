{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ee7cc3",
   "metadata": {},
   "source": [
    "Each module will be given with tests to prove it's doing it's job properly and to save it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549dc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceptable imports for project\n",
    "from torch import empty , cat , arange\n",
    "from torch.nn.functional import fold , unfold\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e3640",
   "metadata": {},
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a484778",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu(object) :\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input) :\n",
    "        self.input = input\n",
    "        self.positif_mask = (input > 0)\n",
    "        return self.positif_mask*(input)\n",
    "    def backward(self, gradwrtoutput) :\n",
    "        self.input.grad = self.positif_mask.int()*gradwrtoutput\n",
    "        return self.input.grad\n",
    "    def param(self) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72872fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = empty(10, 5, 7, 3).normal_()\n",
    "\n",
    "our_M = relu();\n",
    "\n",
    "out = our_M.forward(dummy_input);\n",
    "our_M.backward(out);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e54d090",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34667d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sigmoid(object) :\n",
    "    def forward(self, input) :\n",
    "        self.input = input\n",
    "        self.output = 1/(1 + math.e**(-input))\n",
    "        return  self.output\n",
    "    def backward(self, gradwrtoutput ) :\n",
    "        self.input.grad = self.output * (1-self.output) * gradwrtoutput\n",
    "        return self.input.grad\n",
    "    def param(self) :\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a57219",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = empty(10, 5, 7, 3).normal_()\n",
    "input = empty(10, 5, 7, 3).normal_()\n",
    "\n",
    "model = sigmoid()\n",
    "\n",
    "out = model.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284c140",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0f8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mse(object):\n",
    "    def forward(self, input, target):\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        return (input - target).pow(2).mean()\n",
    "    def backward(self, gradwrtoutput):\n",
    "        self.input.grad = 2*(self.input-self.target)/(self.input.size(-3)*self.input.size(-2)*self.input.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26589d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mse = mse()\n",
    "\n",
    "loss = my_mse.forward(out, target)\n",
    "\n",
    "my_mse.backward(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5381f42",
   "metadata": {},
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b0e564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution(object):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size = (3,3), padding = 0, stride = 1, use_bias = False):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.kernel_size = kernel_size\n",
    "        self.k = self.kernel_size[0]\n",
    "        self.use_bias = use_bias\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel = empty(out_ch, in_ch, self.k, self.k).normal_()\n",
    "        self.bias = empty(out_ch).normal_() if use_bias else torch.zeros(out_ch)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        \n",
    "        self.batch_size = x.size(0)\n",
    "        self.s_in = x.size(-1)\n",
    "        self.s_out = int(math.ceil((x.size(-2)-self.k+1+self.padding*2)/(self.stride)))\n",
    "        \n",
    "        X_unf = unfold(x, kernel_size=(self.k, self.k), padding = self.padding, stride = self.stride)\n",
    "        \n",
    "        self.x = x\n",
    "        self.X_unf = X_unf\n",
    "    \n",
    "        K_expand = self.kernel.view(self.out_ch, -1)\n",
    "        O_expand = K_expand @ X_unf\n",
    "\n",
    "        \n",
    "        O = O_expand.view(self.batch_size, self.out_ch, self.s_out, self.s_out)\n",
    "        return O + self.bias.view(1, -1, 1, 1) if self.use_bias else O\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dL_dO = gradwrtoutput                                       # (B x OUT_CH x SO x SO)\n",
    "        dO_dX = self.kernel                                         # (OUT_CH x IN_CH x SI x SI)\n",
    "\n",
    "        dL_dO_exp = dL_dO.reshape(self.batch_size, self.out_ch, -1) # (B x OUT_CH x (SO x SO))\n",
    "        dO_dX_exp = dO_dX.reshape(self.out_ch,-1).transpose(0,1)    # (OUT_CH x (IN_CH x SI x SI))\n",
    "        dL_dO_unf = dO_dX_exp @ dL_dO_exp                           # (B x (IN_CH x SI x SI) x (SO x SO))\n",
    "\n",
    "        dL_dX = fold(dL_dO_unf, kernel_size = (self.k, self.k), padding = self.padding, stride = self.stride, output_size = (self.s_in, self.s_in))\n",
    "        \n",
    "        # backward wrt weights\n",
    "        dL_dO_exp = dL_dO.transpose(0,1).reshape(self.out_ch, -1) # (OUT_CH x (B x SO x SO))\n",
    "        dO_dF_exp = self.X_unf.transpose(-1, -2).reshape(self.batch_size*self.s_out*self.s_out, -1) # ((B x SO x SO) x (IN_CH x K x K))\n",
    "        dL_dF_exp = dL_dO_exp @ dO_dF_exp # (OUT_CH x  (IN_CH x K x K))\n",
    "        \n",
    "        self.dL_dF = dL_dF_exp.view(self.out_ch, self.in_ch, self.k, self.k)\n",
    "        \n",
    "        # backward wrt bias\n",
    "        if self.use_bias:\n",
    "            dO_dB_exp = 1+0*empty(self.batch_size * (self.s_out) * (self.s_out))\n",
    "            self.dL_dB = dL_dO_exp @ dO_dB_exp\n",
    "        else:\n",
    "            self.dL_dB = None\n",
    "        \n",
    "        return dL_dX, self.dL_dF, self.dL_dB\n",
    "\n",
    "        \n",
    "    def param(self) :\n",
    "        return ((self.kernel, self.dL_dF), (self.bias, self.dL_dB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "751e44f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[[[ 0.0069,  0.0188,  0.9383],\n",
      "          [ 1.1074, -1.1675,  0.0813],\n",
      "          [-0.1009, -0.5582, -1.3208]],\n",
      "\n",
      "         [[-1.6055, -0.7318,  0.2603],\n",
      "          [ 0.8248,  0.8601, -0.8107],\n",
      "          [-0.6488,  1.1049,  1.0795]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4236,  0.0909,  1.2867],\n",
      "          [-0.8907, -1.8950,  0.9544],\n",
      "          [-0.7525, -0.9782, -1.6160]],\n",
      "\n",
      "         [[-0.6253, -0.4612, -0.1277],\n",
      "          [ 1.2424,  0.8762, -1.6619],\n",
      "          [-0.2318,  0.3806, -0.4159]]],\n",
      "\n",
      "\n",
      "        [[[-0.8200,  1.2555, -1.0944],\n",
      "          [ 0.4727, -0.3980,  1.6263],\n",
      "          [-0.8485,  0.7906,  1.0962]],\n",
      "\n",
      "         [[ 1.5212, -0.7339,  0.9833],\n",
      "          [-1.2588,  0.6699,  0.0499],\n",
      "          [ 1.3753,  0.3723,  0.8509]]],\n",
      "\n",
      "\n",
      "        [[[-0.1410, -1.1216,  0.4658],\n",
      "          [ 0.5245,  0.6062,  1.3332],\n",
      "          [-0.3514,  0.4615,  0.3269]],\n",
      "\n",
      "         [[ 1.2005, -1.0177,  0.5376],\n",
      "          [-0.5855,  0.8866,  0.1924],\n",
      "          [-0.4232,  0.6157, -0.8977]]]], requires_grad=True), tensor([[[[-3.6174,  1.4234, -3.6174],\n",
      "          [10.6738,  2.4513, 10.6738],\n",
      "          [-3.6174,  1.4234, -3.6174]],\n",
      "\n",
      "         [[ 2.4851, -0.8437,  2.4851],\n",
      "          [-4.0950, -5.1025, -4.0950],\n",
      "          [ 2.4851, -0.8437,  2.4851]]],\n",
      "\n",
      "\n",
      "        [[[-3.6174,  1.4234, -3.6174],\n",
      "          [10.6738,  2.4513, 10.6738],\n",
      "          [-3.6174,  1.4234, -3.6174]],\n",
      "\n",
      "         [[ 2.4851, -0.8437,  2.4851],\n",
      "          [-4.0950, -5.1025, -4.0950],\n",
      "          [ 2.4851, -0.8437,  2.4851]]],\n",
      "\n",
      "\n",
      "        [[[-3.6174,  1.4234, -3.6174],\n",
      "          [10.6738,  2.4513, 10.6738],\n",
      "          [-3.6174,  1.4234, -3.6174]],\n",
      "\n",
      "         [[ 2.4851, -0.8437,  2.4851],\n",
      "          [-4.0950, -5.1025, -4.0950],\n",
      "          [ 2.4851, -0.8437,  2.4851]]],\n",
      "\n",
      "\n",
      "        [[[-3.6174,  1.4234, -3.6174],\n",
      "          [10.6738,  2.4513, 10.6738],\n",
      "          [-3.6174,  1.4234, -3.6174]],\n",
      "\n",
      "         [[ 2.4851, -0.8437,  2.4851],\n",
      "          [-4.0950, -5.1025, -4.0950],\n",
      "          [ 2.4851, -0.8437,  2.4851]]]], grad_fn=<ViewBackward0>)), (tensor([ 1.3298,  0.4082, -0.1911,  0.5883], requires_grad=True), tensor([32., 32., 32., 32.], grad_fn=<MvBackward0>)))\n"
     ]
    }
   ],
   "source": [
    "# Initial parameters\n",
    "s_1, s_2 = 7,7\n",
    "k_1, k_2 = 3,3\n",
    "bs = 2\n",
    "ch_in, ch_out = 2, 4\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# input tensor \n",
    "X = empty(bs, ch_in, s_1, s_2).normal_()\n",
    "\n",
    "# initialize convolution moduls\n",
    "conv = convolution(ch_in, ch_out, kernel_size = (k_1, k_2), padding = padding, use_bias=True, stride = stride)\n",
    "\n",
    "# get weigts and bias\n",
    "F = conv.kernel\n",
    "B = conv.bias\n",
    "F.requires_grad_()\n",
    "B.requires_grad_()\n",
    "\n",
    "# forward\n",
    "out = conv.forward(X)\n",
    "\n",
    "# backward\n",
    "dL_dX, dL_dF, dL_dB = conv.backward(out/out)\n",
    "\n",
    "print(conv.param())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff235e0",
   "metadata": {},
   "source": [
    "## Transposed Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a44decb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transposed_convolution(object):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size = (3,3), padding = 0, stride = 1, use_bias = False):\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.kernel_size = kernel_size\n",
    "        self.k_1 = self.kernel_size[0]\n",
    "        self.k_2 = self.kernel_size[1]\n",
    "        self.use_bias = use_bias\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.kernel = empty(in_ch, out_ch, self.k_1, self.k_2).normal_()\n",
    "        self.bias = empty(out_ch).normal_() if use_bias else 0*empty(out_ch)\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.batch_size = x.size(0)\n",
    "        self.s1 = self.x.size(-2)\n",
    "        self.s2 = self.x.size(-1)\n",
    "        o1 = (self.s1 - 1)*self.stride + 1 + self.k_1 - 1 - self.padding *2\n",
    "        o2 = (self.s2 - 1)*self.stride + 1 + self.k_2 - 1 - self.padding *2\n",
    "        \n",
    "        self.o1 = o1\n",
    "        self.o2 = o2\n",
    "        \n",
    "        x_exp = x.reshape(self.batch_size, self.in_ch, -1)\n",
    "        K_exp = self.kernel.reshape(self.in_ch,-1).transpose(0,1)\n",
    "        O_unf = K_exp @ x_exp\n",
    "        out = fold(O_unf, kernel_size = (self.k_1, self.k_2), padding = self.padding, stride = self.stride, output_size = (o1,o2))\n",
    "        \n",
    "        return out + self.bias.view(1, -1, 1, 1) if self.use_bias else out\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dL_dO = gradwrtoutput      # B x OUT_CH x SO x SO\n",
    "        dO_dX = self.kernel\n",
    "        \n",
    "        dL_dO_unf = unfold(dL_dO, kernel_size = (self.k_1, self.k_2), padding = self.padding, stride = self.stride)\n",
    "                                   # B x (OUT_CH x K x K) x SI x SI\n",
    "        dO_dX_exp = dO_dX.view(self.in_ch, -1)\n",
    "        dL_dX_exp = dO_dX_exp @ dL_dO_unf\n",
    "        self.dL_dX = dL_dX_exp.view(self.batch_size, self.in_ch, self.s1, self.s2)\n",
    "        \n",
    "        self.dL_dO_unf_K = dL_dO_unf.transpose(0,1).reshape(self.out_ch * self.k_1 * self.k_2, -1).transpose(0,1)\n",
    "                                                                    # (B x SI x SI) x (OUT_CH x K x K)\n",
    "        self.dO_dF_exp = self.x.transpose(0,1).reshape(self.in_ch, -1)   # IN_CH x (B x SI x SI)\n",
    "        self.dL_dF_exp = self.dO_dF_exp @ self.dL_dO_unf_K                         # IN_CH x (OUT_CH x K x K)                                                                       \n",
    "        self.dL_dF = self.dL_dF_exp.view(self.in_ch, self.out_ch, self.k_1, self.k_2)  # OUT_CH x IN_CH x K x K\n",
    "        \n",
    "        dL_dO_exp = dL_dO.transpose(0,1).reshape(self.out_ch, -1)\n",
    "        dO_dB_exp = 1+0*empty(self.batch_size * (self.o1) * (self.o2))\n",
    "        self.dL_dB = dL_dO_exp @ dO_dB_exp\n",
    "        \n",
    "        return self.dL_dX, self.dL_dF, self.dL_dB\n",
    "        \n",
    "    def param(self) :\n",
    "        return ((self.kernel, self.dL_dF), (self.bias, self.dL_dB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d8dbb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((tensor([[[[ 1.5161,  0.0515,  1.3460],\n",
      "          [ 0.5884,  0.2913, -0.0336],\n",
      "          [ 0.8021,  0.0519,  0.2712]],\n",
      "\n",
      "         [[-0.0918, -1.6271,  0.1195],\n",
      "          [-1.4657,  1.0647,  0.1597],\n",
      "          [ 0.7131, -0.6451,  1.4112]]],\n",
      "\n",
      "\n",
      "        [[[-0.3635,  0.4464,  0.7092],\n",
      "          [-1.9496,  0.1352,  0.2133],\n",
      "          [ 2.3279,  1.4274, -1.0688]],\n",
      "\n",
      "         [[-0.6493, -1.6794,  1.2861],\n",
      "          [-1.0077,  0.1461,  1.0481],\n",
      "          [-1.0401, -0.1514, -1.1340]]],\n",
      "\n",
      "\n",
      "        [[[-0.0303,  0.2018, -1.4957],\n",
      "          [ 0.7967,  1.9318,  1.6917],\n",
      "          [-1.1244, -0.2905, -0.9547]],\n",
      "\n",
      "         [[ 0.6784,  0.2066, -0.1596],\n",
      "          [ 1.2293,  1.6930,  0.9298],\n",
      "          [-0.8403,  2.4258, -0.1050]]]]), tensor([[[[-6.0013,  2.7847, 13.7594],\n",
      "          [-5.7279,  2.0224, 10.8056],\n",
      "          [-5.0121,  0.6958,  3.7170]],\n",
      "\n",
      "         [[-6.0013,  2.7847, 13.7594],\n",
      "          [-5.7279,  2.0224, 10.8056],\n",
      "          [-5.0121,  0.6958,  3.7170]]],\n",
      "\n",
      "\n",
      "        [[[-5.4770,  3.6271,  2.9972],\n",
      "          [ 5.9746, 12.7863,  3.2273],\n",
      "          [ 9.7780, 16.5616,  5.4255]],\n",
      "\n",
      "         [[-5.4770,  3.6271,  2.9972],\n",
      "          [ 5.9746, 12.7863,  3.2273],\n",
      "          [ 9.7780, 16.5616,  5.4255]]],\n",
      "\n",
      "\n",
      "        [[[-5.1851, -5.0246, -7.5559],\n",
      "          [ 0.5362,  5.3129, -1.2996],\n",
      "          [ 4.3016,  0.6973, -3.7971]],\n",
      "\n",
      "         [[-5.1851, -5.0246, -7.5559],\n",
      "          [ 0.5362,  5.3129, -1.2996],\n",
      "          [ 4.3016,  0.6973, -3.7971]]]])), (tensor([-0.5439,  0.5553]), tensor([45., 45.])))\n"
     ]
    }
   ],
   "source": [
    "padding = 2\n",
    "stride = 1\n",
    "in_ch = 3\n",
    "out_ch = 2\n",
    "batch_size = 5\n",
    "si = 5\n",
    "\n",
    "test = transposed_convolution(in_ch,out_ch, kernel_size = (3,3), use_bias = True, padding = padding, stride = stride)\n",
    "kernel_test = empty((in_ch,out_ch,3,3)).normal_() #torch.ones((1,1,3,3))\n",
    "bias_test = empty((out_ch)).normal_()\n",
    "test.kernel = kernel_test\n",
    "test.bias = bias_test\n",
    "input = empty((batch_size,in_ch,si,si)).normal_() #torch.ones((1,1,5,5))\n",
    "output = test.forward(input)\n",
    "\n",
    "dL_dX, dL_dF, dL_dB = test.backward(output/output)\n",
    "print(test.param())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d3761e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
